{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the workshop Intro to HPC for Deep Learning \ud83d\udc4b.</p> <p>This workshop is designed to help deep learning practitioners get started on NeSI\u2019s high performance computing platform.</p> <p>We won\u2019t be teaching how to do the cool bleeding-edge deep learning, but we will answer existential questions such as \u201cHow do I get package X to find the GPU Y and run with CUDA version Z \ud83d\ude31?\u201d.</p> <p>The following topics will be covered:</p> <ul> <li>Installation: use preinstalled deep learning packages or install it yourself,</li> <li>Batch jobs: submit a batch job using a GPU and make sure to use it,</li> <li>Monitoring: track progression of model training and GPU usage.</li> </ul> <p>During this workshop, we will be using the Jupyter-on-NeSI service. Let\u2019s now head to the Workshop Setup section to get started on the platform.</p>"},{"location":"install/","title":"Software Installation","text":"<p>In this section, we will explore multiple ways to install Deep Learning packages on the NeSI platform. We will focus on Python packages although many advice apply to other languages.</p>"},{"location":"install/#environment-modules","title":"Environment Modules","text":"<p>The first method to install software is\u2026 to not install them but use those preinstalled \ud83d\ude01.</p> <p>On NeSI\u2019s platform, we use Environment Modules to provide software in a flexible way, allowing users to load specific versions and their dependencies.</p> <p>Here we will use TensorFlow as an example. So let\u2019s first assert that this package is not available in the default environment:</p> <pre><code>python3 -c \"import tensorflow; print(tensorflow.__version__)\"\n</code></pre> output <pre><code>Traceback (most recent call last):\nFile \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'tensorflow'\n</code></pre> <p>Next, let\u2019s check if there is a TensorFlow envionment module on the platform, using the search command <code>module spider</code>:</p> <pre><code>module spider TensorFlow\n</code></pre> output <pre><code>-------------------------------------------------------------------------------------------\n  TensorFlow:\n-------------------------------------------------------------------------------------------\n    Description:\n      An open-source software library for Machine Intelligence\n\n     Versions:\n        TensorFlow/2.0.1-gimkl-2018b-Python-3.8.1\n        TensorFlow/2.2.0-gimkl-2018b-Python-3.8.1\n        TensorFlow/2.2.2-gimkl-2018b-Python-3.8.1\n        TensorFlow/2.2.3-gimkl-2018b-Python-3.8.1\n        TensorFlow/2.3.1-gimkl-2020a-Python-3.8.2\n        TensorFlow/2.4.1-gimkl-2020a-Python-3.8.2\n        TensorFlow/2.8.2-gimkl-2022a-Python-3.10.5\n        TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n\n     Other possible modules matches:\n        tensorflow\n\n-------------------------------------------------------------------------------------------\n  To find other possible module matches do:\n      module -r spider '.*TensorFlow.*'\n\n-------------------------------------------------------------------------------------------\n  For detailed information about a specific \"TensorFlow\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider TensorFlow/2.8.2-gimkl-2022a-Python-3.10.5\n-------------------------------------------------------------------------------------------\n</code></pre> <p>Success! There are a couple of versions, let\u2019s load version 2.13.0.</p> <pre><code>module purge\nmodule load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n</code></pre> <p>Tips</p> <p>The <code>module purge</code> command ensures that we start from a clean state.</p> <p>Always specify the version of a module, this will save you \u2013 and people helping you \u2013 a lot of time when debugging installation issues \ud83d\ude09.</p> <p>We can now check that TensorFlow is available from a Python interpreter:</p> <pre><code>python3 -c \"import tensorflow; print(tensorflow.__version__)\"\n</code></pre> output <pre><code>2024-03-11 12:29:51.145566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2.13.0\n</code></pre> <p>It is also interesting to list all the modules loaded as dependencies from TensorFlow:</p> <pre><code>module list\n</code></pre> output <pre><code>Currently Loaded Modules:\n  1) XALT/minimal\n  2) slurm\n  3) NeSI                                                   (S)\n  4) GCCcore/11.3.0\n  5) zlib/1.2.11-GCCcore-11.3.0\n  6) binutils/2.38-GCCcore-11.3.0\n  7) GCC/11.3.0\n  8) libpmi/2-slurm\n  9) numactl/2.0.14-GCC-11.3.0\n 10) UCX/1.12.1-GCC-11.3.0\n 11) impi/2021.5.1-GCC-11.3.0\n 12) AlwaysIntelMKL/1.0\n 13) imkl/2022.0.2\n 14) gimpi/2022a\n 15) imkl-FFTW/2022.0.2-gimpi-2022a\n 16) gimkl/2022a\n 17) bzip2/1.0.8-GCCcore-11.3.0\n 18) XZ/5.2.5-GCCcore-11.3.0\n 19) libpng/1.6.37-GCCcore-11.3.0\n 20) freetype/2.11.1-GCCcore-11.3.0\n 21) Szip/2.1.1-GCCcore-11.3.0\n 22) HDF5/1.12.2-gimpi-2022a\n 23) libjpeg-turbo/2.1.3-GCCcore-11.3.0\n 24) ncurses/6.2-GCCcore-11.3.0\n 25) libreadline/8.1-GCCcore-11.3.0\n 26) libxml2/2.9.10-GCCcore-11.3.0\n 27) libxslt/1.1.34-GCCcore-11.3.0\n 28) cURL/7.83.1-GCCcore-11.3.0\n 29) netCDF/4.8.1-gimpi-2022a\n 30) SQLite/3.36.0-GCCcore-11.3.0\n 31) METIS/5.1.0-GCC-11.3.0\n 32) GMP/6.2.1-GCCcore-11.3.0\n 33) MPFR/4.1.0-GCC-11.3.0\n 34) SuiteSparse/5.13.0-gimkl-2022a\n 35) Tcl/8.6.10-GCCcore-11.3.0\n 36) Tk/8.6.10-GCCcore-11.3.0\n 37) ZeroMQ/4.3.4-GCCcore-11.3.0\n 38) OpenSSL/1.1.1k-GCCcore-11.3.0\n 39) Python/3.11.3-gimkl-2022a\n 40) CUDA/11.8.0\n 41) cuDNN/8.6.0.163-CUDA-11.8.0\n 42) TensorRT/8.6.1.6-gimkl-2022a-Python-3.11.3-CUDA-11.8.0\n 43) NCCL/2.16.5-CUDA-11.8.0\n 44) TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n</code></pre> <p>You will note that CUDA and cuDNN are loaded, and the right version to make it just works.</p> <p>Modules are prepared with \u2764\ufe0f by NeSI\u2019s Research Support Team. If you need a package or a version that is not available, do not hesitate to contact us at support@nesi.org.nz to get it installed for you.</p> <p>See also</p> <p>The TensorFlow support page also hightlights how to use Python virtual environments, with and without reusing system packages provided by an environment module.</p>"},{"location":"install/#conda-environment","title":"Conda Environment","text":"<p>If you want to be fully in control of you software stack, there are handful of solutions. One of them is to use the Conda package manager. Conda lets you create \u201cconda environments\u201d, which allow you to install a set of packages in isolation.</p> <p>Before using <code>conda</code>, we need to load the corresponding environment module:</p> <pre><code>module purge\nmodule load Miniconda3/23.10.0-1\nconda -V\n</code></pre> output <pre><code>conda 23.10.0\n</code></pre> <p>Because deep learning frameworks tend to be\u2026 fairly large \ud83d\ude05, we need to be careful how we install these packages to avoid wasting storage space and locking our home storage (20GB maximum). We will set a default folder to store the package cache in <code>nobackup</code> storage:</p> <pre><code>conda config --add pkgs_dirs /nesi/nobackup/nesi99991/conda_pkgs/$USER\n</code></pre> <p>You only need to do this once.</p> <p>Tip</p> <p>Using <code>nobackup</code> storage will ensure that downloaded packages will be automatically removed after 120 days. This won\u2019t affect the conda environments where these packages are installed.</p> <p>Next, let\u2019s create a conda environment from a given definition file <code>environment.yml</code>, which includes an installation of TensorFlow:</p> <pre><code>export PIP_NO_CACHE_DIR=1\nexport PYTHONNOUSERSITE=1\nconda env create -f /nesi/project/nesi99991/introhpc2403/environment.yml -p /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n</code></pre> output <pre><code>Channels:\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\nDownloading and Extracting Packages:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nInstalling pip dependencies: - Ran pip subprocess with arguments:\n['/nesi/nobackup/nesi99991/introhpc2403/riom/venv/bin/python', '-m', 'pip', 'install', '-U', '-r', '/nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt', '--exists-action=b']\nPip subprocess output:\nCollecting tensorflow==2.12.0 (from -r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting tensorflow-datasets (from -r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading tensorflow_datasets-4.9.4-py3-none-any.whl.metadata (9.2 kB)\nCollecting absl-py&gt;=1.0.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting astunparse&gt;=1.6.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting flatbuffers&gt;=2.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\nCollecting gast&lt;=0.4.0,&gt;=0.2.1 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\nCollecting google-pasta&gt;=0.1.1 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\nCollecting grpcio&lt;2.0,&gt;=1.24.3 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting h5py&gt;=2.9.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nCollecting jax&gt;=0.3.15 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading jax-0.4.25-py3-none-any.whl.metadata (24 kB)\nCollecting keras&lt;2.13,&gt;=2.12.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting libclang&gt;=13.0.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\nCollecting numpy&lt;1.24,&gt;=1.22 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nCollecting opt-einsum&gt;=2.3.2 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nCollecting packaging (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nRequirement already satisfied: setuptools in /scale_wlg_nobackup/filesets/nobackup/nesi99991/introhpc2403/riom/venv/lib/python3.10/site-packages (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1)) (69.1.1)\nCollecting six&gt;=1.12.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorboard&lt;2.13,&gt;=2.12 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorflow-estimator&lt;2.13,&gt;=2.12.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting termcolor&gt;=1.1.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\nCollecting typing-extensions&gt;=3.6.6 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting wrapt&lt;1.15,&gt;=1.11.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.23.1 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nCollecting click (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\nCollecting dm-tree (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nCollecting etils&gt;=0.9.0 (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading etils-1.7.0-py3-none-any.whl.metadata (6.4 kB)\nCollecting promise (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading promise-2.3.tar.gz (19 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting psutil (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nCollecting requests&gt;=2.19.0 (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting tensorflow-metadata (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\nCollecting toml (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\nCollecting tqdm (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 57.6/57.6 kB 244.9 MB/s eta 0:00:00\nCollecting array-record&gt;=0.5.0 (from tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (503 bytes)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /scale_wlg_nobackup/filesets/nobackup/nesi99991/introhpc2403/riom/venv/lib/python3.10/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1)) (0.42.0)\nCollecting fsspec (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting importlib_resources (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading importlib_resources-6.1.3-py3-none-any.whl.metadata (3.9 kB)\nCollecting zipp (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting ml-dtypes&gt;=0.2.0 (from jax&gt;=0.3.15-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting scipy&gt;=1.9 (from jax&gt;=0.3.15-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.4/60.4 kB 274.1 MB/s eta 0:00:00\nCollecting charset-normalizer&lt;4,&gt;=2 (from requests&gt;=2.19.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna&lt;4,&gt;=2.5 (from requests&gt;=2.19.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\nCollecting urllib3&lt;3,&gt;=1.21.1 (from requests&gt;=2.19.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting certifi&gt;=2017.4.17 (from requests&gt;=2.19.0-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting google-auth&lt;3,&gt;=1.6.3 (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading google_auth-2.28.2-py2.py3-none-any.whl.metadata (4.7 kB)\nCollecting google-auth-oauthlib&lt;1.1,&gt;=0.5 (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting markdown&gt;=2.6.8 (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\nCollecting tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\nCollecting werkzeug&gt;=1.0.1 (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting absl-py&gt;=1.0.0 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting googleapis-common-protos&lt;2,&gt;=1.52.0 (from tensorflow-metadata-&gt;tensorflow-datasets-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 2))\n  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 (from tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\nCollecting cachetools&lt;6.0,&gt;=2.0.0 (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\nCollecting pyasn1-modules&gt;=0.2.1 (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\nCollecting rsa&lt;5,&gt;=3.1.4 (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\nCollecting requests-oauthlib&gt;=0.7.0 (from google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading requests_oauthlib-1.4.0-py2.py3-none-any.whl.metadata (11 kB)\nCollecting MarkupSafe&gt;=2.1.1 (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting pyasn1&lt;0.6.0,&gt;=0.4.6 (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\nCollecting oauthlib&gt;=3.0.0 (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;1.1,&gt;=0.5-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow==2.12.0-&gt;-r /nesi/project/nesi99991/introhpc2403/condaenv.gvo4091q.requirements.txt (line 1))\n  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\nDownloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 585.9/585.9 MB 15.4 MB/s eta 0:00:00\nDownloading tensorflow_datasets-4.9.4-py3-none-any.whl (5.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.1/5.1 MB 11.0 MB/s eta 0:00:00\nDownloading array_record-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.0/3.0 MB 248.0 MB/s eta 0:00:00\nDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nDownloading etils-1.7.0-py3-none-any.whl (152 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 152.4/152.4 kB 240.3 MB/s eta 0:00:00\nDownloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\nDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nDownloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 57.5/57.5 kB 219.7 MB/s eta 0:00:00\nDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.5/5.5 MB 249.4 MB/s eta 0:00:00\nDownloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.8/4.8 MB 294.5 MB/s eta 0:00:00\nDownloading jax-0.4.25-py3-none-any.whl (1.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.8/1.8 MB 315.6 MB/s eta 0:00:00\nDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 301.3 MB/s eta 0:00:00\nDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22.9/22.9 MB 21.8 MB/s eta 0:00:00\nDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 17.1/17.1 MB 9.6 MB/s eta 0:00:00\nDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 65.5/65.5 kB 204.2 MB/s eta 0:00:00\nDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.6/62.6 kB 171.2 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.6/5.6 MB 23.6 MB/s eta 0:00:00\nDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 440.7/440.7 kB 216.5 MB/s eta 0:00:00\nDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.1/5.1 MB 149.0 MB/s eta 0:00:00\nDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\nDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\nDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.9/77.9 kB 189.5 MB/s eta 0:00:00\nDownloading click-8.1.7-py3-none-any.whl (97 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 97.9/97.9 kB 193.6 MB/s eta 0:00:00\nDownloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 152.8/152.8 kB 203.6 MB/s eta 0:00:00\nDownloading packaging-24.0-py3-none-any.whl (53 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 53.5/53.5 kB 186.3 MB/s eta 0:00:00\nDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 288.2/288.2 kB 230.6 MB/s eta 0:00:00\nDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\nDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 126.5/126.5 kB 203.7 MB/s eta 0:00:00\nDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 233.4 MB/s eta 0:00:00\nDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\nDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.3/78.3 kB 206.3 MB/s eta 0:00:00\nDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.8/163.8 kB 206.6 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 142.1/142.1 kB 138.8 MB/s eta 0:00:00\nDownloading google_auth-2.28.2-py2.py3-none-any.whl (186 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 186.9/186.9 kB 197.5 MB/s eta 0:00:00\nDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.1/229.1 kB 212.7 MB/s eta 0:00:00\nDownloading idna-3.6-py3-none-any.whl (61 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.6/61.6 kB 190.2 MB/s eta 0:00:00\nDownloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 103.9/103.9 kB 198.9 MB/s eta 0:00:00\nDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.2/2.2 MB 242.3 MB/s eta 0:00:00\nDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.4/38.4 MB 232.2 MB/s eta 0:00:00\nDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\nDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.1/121.1 kB 243.5 MB/s eta 0:00:00\nDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 226.7/226.7 kB 267.2 MB/s eta 0:00:00\nDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 170.9/170.9 kB 274.0 MB/s eta 0:00:00\nDownloading importlib_resources-6.1.3-py3-none-any.whl (34 kB)\nDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\nDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\nDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 181.3/181.3 kB 287.5 MB/s eta 0:00:00\nDownloading requests_oauthlib-1.4.0-py2.py3-none-any.whl (24 kB)\nDownloading rsa-4.9-py3-none-any.whl (34 kB)\nDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 151.7/151.7 kB 276.9 MB/s eta 0:00:00\nDownloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.9/84.9 kB 271.3 MB/s eta 0:00:00\nBuilding wheels for collected packages: promise\n  Building wheel for promise (setup.py): started\n  Building wheel for promise (setup.py): finished with status 'done'\n  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=bd49a38bd62d2431128bcb8637d955e7fa693744446508c70e8e87281f613a8d\n  Stored in directory: /dev/shm/jobs/44320689/pip-ephem-wheel-cache-_lijtdvo/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\nSuccessfully built promise\nInstalling collected packages: libclang, flatbuffers, dm-tree, zipp, wrapt, urllib3, typing-extensions, tqdm, toml, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pyasn1, psutil, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, importlib_resources, idna, grpcio, gast, fsspec, etils, click, charset-normalizer, certifi, cachetools, absl-py, werkzeug, scipy, rsa, requests, pyasn1-modules, promise, opt-einsum, ml-dtypes, h5py, googleapis-common-protos, google-pasta, astunparse, tensorflow-metadata, requests-oauthlib, jax, google-auth, google-auth-oauthlib, array-record, tensorboard, tensorflow-datasets, tensorflow\nSuccessfully installed MarkupSafe-2.1.5 absl-py-1.4.0 array-record-0.5.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 dm-tree-0.1.8 etils-1.7.0 flatbuffers-24.3.7 fsspec-2024.2.0 gast-0.4.0 google-auth-2.28.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 googleapis-common-protos-1.63.0 grpcio-1.62.1 h5py-3.10.0 idna-3.6 importlib_resources-6.1.3 jax-0.4.25 keras-2.12.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.3.2 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-24.0 promise-2.3 protobuf-3.20.3 psutil-5.9.8 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.4.0 rsa-4.9 scipy-1.12.0 six-1.16.0 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.0 tensorflow-datasets-4.9.4 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-metadata-1.14.0 termcolor-2.4.0 toml-0.10.2 tqdm-4.66.2 typing-extensions-4.10.0 urllib3-2.2.1 werkzeug-3.0.1 wrapt-1.14.1 zipp-3.17.0\n\ndone\n#\n# To activate this environment, use\n#\n#     $ conda activate /nesi/nobackup/nesi99991/introhpc2403/riom/venv\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> <p>This will take a bit of time, so it could be a good time for a \ud83c\udf75 break.</p> <p>Note that we used the <code>-p</code> option to specify the location of our conda environment and avoid installing it in our home folder (what <code>-n</code> would normally do).</p> <p>Warning</p> <p>Setting the variable <code>PIP_NO_CACHE_DIR=1</code> ensures that <code>pip</code> doesn\u2019t cache packages in your home folder.</p> <p>Setting the variable <code>PYTHONNOUSERSITE=1</code> is necessary to ensure that <code>pip</code> doesn\u2019t look into user\u2019s home folder for locally installed Python packages (using <code>pip install --user</code>). This will ensure that the conda environment is really isolated from the system and more reproducible.</p> <p>At last, we have our conda environment ready, let\u2019s activate it to see if TensorFlow is properly installed:</p> <pre><code>conda activate /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n</code></pre> output <pre><code>usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...\nconda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'content-trust', 'doctor', 'repoquery', 'env')\n</code></pre> <p>Oh no! we cannot activate the environment \ud83d\ude28.</p> <p>The reason is that we need to one extra step to fully configure conda in addition to loading the environment module:</p> <pre><code>source $(conda info --base)/etc/profile.d/conda.sh\n</code></pre> <code>conda init</code>, aka our nightmare <p>You might be tempted to use <code>conda init</code> here. Do not. Seriously. Please \ud83d\ude4f. This will insert a small piece of code in your <code>~/.bashrc</code> file that is fine on an individual computer but interacts poorly with the environment module system on the HPC platform.</p> <p>If you have used this command, you can edit <code>~/.bashrc</code> using a terminal based editor like <code>nano</code> and remove the offending piece of code: <pre><code>nano ~/.bashrc\n</code></pre> and start a new terminal.</p> <p>Ok, now we can finally test our installation:</p> <pre><code>conda activate /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\npython3 -c \"import tensorflow; print(tensorflow.__version__)\"\n</code></pre> output <pre><code>2024-03-12 00:10:16.189564: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-03-12 00:10:17.419378: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-03-12 00:10:17.420215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-12 00:10:23.738598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n2.12.0\n</code></pre> <p>Warning</p> <p>Using Conda on the HPC has numerous traps, we tried to highlight most of them here. In case of doubt, please check our dedicated support page.</p>"},{"location":"install/#apptainer-container","title":"Apptainer Container","text":"<p>The last option we will explore in this section is containers. Containers provide a very lightweight way to run a complete Linux virtual machine isolated from the host environment, with little performance impact.</p> <p>Docker is the most well known container runtime but many others exist. For historical and security reasons, alternative runtimes have been developed in the HPC world, as we can\u2019t give administrator rights to users. On NeSI, we use Singularity and Apptainer, which are compatible with Docker containers.</p> <p>First, we need to load the corresponding environment module:</p> <pre><code>module purge\nmodule load Apptainer/1.2.2\n</code></pre> <p>Next, we will fetch a TensorFlow container from NVIDIA NGC Catalog. Here is how one would convert the Docker container as an Apptainer container while downloading it:</p> <pre><code>apptainer pull tensorflow-24.02.sif docker://nvcr.io/nvidia/tensorflow:24.02-tf2-py3\n</code></pre> <p>Warning</p> <p>Please do not run this <code>apptainer pull</code> command, it will create a 7GB file and take quite a lot of time.</p> <p>We have already pulled the container for you as <code>/nesi/project/nesi99991/introhpc2403/tensorflow-24.02.sif</code>.</p> <p>There are multiple ways to interact with a container. Here we will execute a command in the container using the <code>apptainer exec</code> command:</p> <pre><code>apptainer exec /nesi/project/nesi99991/introhpc2403/tensorflow-24.02.sif \\\n    python3 -c \"import tensorflow; print(tensorflow.__version__)\"\n</code></pre> output <pre><code>13:4: not a valid test operator: (\n13:4: not a valid test operator: \n2024-03-12 02:00:43.405327: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-12 02:00:43.407144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-12 02:00:43.572380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-03-12 02:00:43.847914: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2.15.0\n</code></pre> <p>See also</p> <p>Here we looked very briefly at containers. If you want to know more about this topic, please check our support page about Singularity, which is also applicable to Apptainer. We also have a specific page on how to build a container on NeSI, which is a more advanced topic.</p> <p>In the next section, we will see how to submit jobs to train a deep learning model using a GPU.</p>"},{"location":"interact/","title":"Interactive Access","text":"<p>TODO srun (/!\\ hgx not working from jupyter terminal, switch to login node)</p> <p>TODO jupyter notebook</p> <p>TODO nesi-add-kernel</p> <p>TODO papermill</p> <p>TODO link to webinar and docs?</p>"},{"location":"monitor/","title":"Monitoring","text":"<p>In this section, we will explore multiple ways to monitor the activity of a job while it is running.</p>"},{"location":"monitor/#live-log-file-monitoring","title":"Live log file monitoring","text":"<p>So far, we looked at the content of the log file at the end using the <code>cat</code> command. Wouldn\u2019t it be great if we could see in \u201creal time\u201d the content of this file?</p> <p>To achieve this, we will use the <code>tail</code> command with a little twist \ud83e\ude84.</p> <p>The <code>tail</code> command prints by default the last lines of a file. Let\u2019s try this on one of our log files, for example (replace <code>44349268</code> with any of your jobs ID):</p> <pre><code>tail slurm-44349268.out\n</code></pre> output <pre><code>Epoch 2/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 1.1781 - accuracy: 0.5816 - val_loss: 1.1526 - val_accuracy: 0.5874\nEpoch 3/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 1.0207 - accuracy: 0.6384 - val_loss: 1.0523 - val_accuracy: 0.6281\nEpoch 4/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.9263 - accuracy: 0.6742 - val_loss: 0.9380 - val_accuracy: 0.6695\nEpoch 5/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.8484 - accuracy: 0.7035 - val_loss: 0.9007 - val_accuracy: 0.6905\n313/313 - 0s - loss: 0.9007 - accuracy: 0.6905 - 403ms/epoch - 1ms/step\ntest accuracy: 0.690500020980835\n</code></pre> <p>But using the <code>-f</code> option, the <code>tail</code> command will keep watching the file and print updates to the file as they happen.</p> <p>Let\u2019s run a new job to illustrate this, using one of our job scripts from the previous section:</p> <pre><code>sbatch train_model_env.sl\n</code></pre> output <pre><code>Submitted batch job 44392266\n</code></pre> <p>Double check that your job is running with <code>squeue --me</code>:</p> <pre><code>squeue --me\n</code></pre> output <pre><code>JOBID         USER     ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)    \n44348624      riom     nesi99999 spawner-jupy   2      4G interac 2024-03-12T1     1:58:27 RUNNING  wbn001              \n44392266      riom     nesi99991 train_model_   2      8G hgx     2024-03-12T2        9:56 RUNNING  wmg003\n</code></pre> <p>And then open the log file using <code>tail -f</code> (replace <code>44392266</code> with your job ID):</p> <pre><code>tail -f slurm-44392266.out\n</code></pre> <p>Warning</p> <p>The <code>tail -f</code> command will not end, even once the job has finished. To get the command line prompt back, you need to interrupt it using Ctrl+C.</p>"},{"location":"monitor/#tensorboard","title":"Tensorboard","text":"<p>TensorBoard is a powerful visualisation tool that let\u2019s you track  and display various metrics while a model is being trained. It\u2019s a web application, meaning that its interface is accessible via your web browser.</p> <p>To use it, you need to make sure that your code save logs in the right format. In our example script <code>training_model.py</code>, this is handled by the TensorBoard callback:</p> <pre><code>tensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=output_folder / \"logs\", histogram_freq=1\n)\nhistory = model.fit(\n    train_images,\n    train_labels,\n    epochs=5,\n    validation_data=(test_images, test_labels),\n    callbacks=[tensorboard_callback]\n)\n</code></pre> <p>Here, we decided to save the TensorBoard logs in a sub-folder <code>logs</code> of the job result folder.</p> <p>Before running it on a live job, let\u2019s use it to look at the metrics from a previous job (replace <code>44393779_train_model_env.sl</code> with one of your results folder):</p> <pre><code>module purge\nmodule load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\ntensorboard --logdir 44393779_train_model_env.sl/logs/\n</code></pre> output <pre><code>2024-03-13 00:04:50.888636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-13 00:04:53.655259: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n</code></pre> <p>Now TensorBoard is running locally in your session, on port 6006 (default value). But you are using a remote session, so opening http://localhost:6006/ in your browser won\u2019t work \ud83d\ude22.</p> <p>Fortunately, JupyterLab \u2013 the interface you are currently using to access the platform \u2013 can proxy web applications if you access it via a special proxy url:</p> <pre><code>https://jupyter.nesi.org.nz/user-redirect/proxy/PORTNUMBER/\n</code></pre> <p>for an application running on port <code>PORTNUMBER</code>.</p> <p>In our case, this would be https://jupyter.nesi.org.nz/user-redirect/proxy/6006/.</p> <p>Warning</p> <p>The final slash <code>/</code> character in the url is important. If you forget it, you will see a white empty page.</p> <p>Failure</p> <p>If someone else is already running an application on port 6006 on the same node as you, TensorBoard will crashe with the following message:</p> <pre><code>Address already in use\nPort 6006 is in use by another program. Either identify and stop that program, or start the server with a different port.\n</code></pre> <p>To avoid this issue, select another port using the <code>--port</code> option, for example using port 25601 (replace <code>44393779_train_model_env.sl</code> with one of your results folder):</p> <pre><code>tensorboard --logdir 44393779_train_model_env.sl/logs/ --port 25601\n</code></pre> <p>and change the proxy url accordingly.</p> <p>Like <code>tail -f</code>, the <code>tensorboard</code> command does not end by itself. Press Ctrl+C to interrupt it and get the command line prompt back.</p> <p>Info</p> <p>To illustrate how to use TensorBoard in live mode, we need to increase the runtime of our Slurm jobs.</p> <p>Edit your copy of <code>train_model.py</code> to increase the number of epochs from 5 to 30.</p> <pre><code>nano /nesi/project/nesi99991/introhpc2403/$USER/train_model.py\n</code></pre> <p>Interestingly, TensorBoard can be used while a job is running and writing information in its corresponding <code>logs</code> folder.</p> <p>Let\u2019s give it a try! Submit a new training job:</p> <pre><code>sbatch train_model_env.sl\n</code></pre> output <pre><code>Submitted batch job 44394400\n</code></pre> <p>Then, as soon as the output folder is created, start the <code>tensorboard</code> command (replace <code>44394400_train_model_env.sl</code> with your job result folder):</p> <pre><code>tensorboard --logdir 44394400_train_model_env.sl/logs/\n</code></pre> <p>and open the proxy url https://jupyter.nesi.org.nz/user-redirect/proxy/6006/.</p> <p>To enable live update in the web interface, click on the cogwheel icon (top right) and then tick the option Reload Data.</p> <p></p> <p>Info</p> <p>Make sure to edit your copy of <code>train_model.py</code> to decrease the number of epochs back to 5.</p> <pre><code>nano /nesi/project/nesi99991/introhpc2403/$USER/train_model.py\n</code></pre>"},{"location":"monitor/#gpu-usage","title":"GPU usage","text":"<p>Another aspect which is import to monitor is how well the GPU is used while running the training code. This can help diagnose simple errors (e.g. the code is not running on the GPU) as weel as tune some hyperparameters (e.g. increase the batch size).</p> <p>Here, we will rely on <code>nvidia-smi</code> to collect information in a .csv file, while our job is running.</p> <p>Edit one of your job submission scripts, for example:</p> <pre><code>nano train_model_env.sl\n</code></pre> <p>and insert the following command at the beginning instead of <code>nvidia-smi</code>:</p> <pre><code># monitor GPU usage\nnvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,memory.total \\\n    --format=csv,nounits -l 5 &gt; \"gpustats-${SLURM_JOB_ID}.csv\" &amp;\n</code></pre> train_model_env.sl (monitored) <pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=8GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# monitor GPU usage\nnvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.used,memory.total \\\n    --format=csv,nounits -l 5 &gt; \"gpustats-${SLURM_JOB_ID}.csv\" &amp;\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# load required environment modules\nmodule purge\nmodule load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>This will make  <code>nvidia-smi</code> output every 5 seconds (<code>-l 5</code>) a series of measures to a .csv file.</p> <p>See also</p> <p>More details about available metrics and their meaning is available in Nvidia documentation.</p> <p>Then, let\u2019s submit a job:</p> <pre><code>sbatch train_model_env.sl\n</code></pre> output <pre><code>Submitted batch job 44409149\n</code></pre> <p>And, once the job finished, look at the generated .csv (replace <code>44409149</code> with your job ID):</p> <pre><code>cat gpustats-44409149.csv\n</code></pre> output <pre><code>timestamp, utilization.gpu [%], utilization.memory [%], memory.used [MiB], memory.total [MiB]\n2024/03/13 07:04:28.836, 0, 0, 0, 81920\n2024/03/13 07:04:33.838, 0, 0, 445, 81920\n2024/03/13 07:04:38.839, 28, 0, 80257, 81920\n2024/03/13 07:04:43.843, 32, 0, 80257, 81920\n2024/03/13 07:04:48.844, 30, 0, 80257, 81920\n2024/03/13 07:04:53.846, 26, 0, 80257, 81920\n2024/03/13 07:04:58.849, 0, 0, 80257, 81920\n</code></pre> <p>This is a nice pile of number, wouldn\u2019t be nicer to turn it into a plot \ud83d\ude09?</p> <p>Let\u2019s do this using a little bit of Python code. Open a new Console<sup>1</sup> in JupyterLab and use the following commands (replace the file path with your .csv file):</p> <pre><code>import pandas as pd\nstatsfile = \"/nesi/project/nesi99991/introhpc2403/riom/gpustats-44409149.csv\"\ndset = pd.read_csv(statsfile, parse_dates=True, index_col=\"timestamp\")\ndset.plot(subplots=True, figsize=(10, 9), grid=True)\n</code></pre> <p></p> <ol> <li> <p>In the File menu, under New, click on Console and then select the kernel <code>Python 3.10.5 (gimkl-2022a)</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"references/","title":"References","text":"<ul> <li>Slurm Reference Sheet</li> <li>Available GPUs on NeSI</li> <li>Mahuika Slurm partitions</li> <li>GPU usage support page</li> <li>Miniconda support page</li> <li>TensorFlow support page</li> <li>TensorFlow compatibility matrix</li> <li>Singularity support page</li> <li>Build a container on NeSI</li> <li>Jupyter kernel tool support page</li> </ul>"},{"location":"setup/","title":"Workshop Setup","text":"<p>During this workshop we will be running the material on the NeSI platform, using the Jupyter-on-NeSI service. This section will walk you through the steps to start a session and checking that the workshop material is ready for you.</p>"},{"location":"setup/#connect-to-jupyter-on-nesi","title":"Connect to Jupyter on NeSI","text":"<ol> <li>Open https://jupyter.nesi.org.nz in your web browser.</li> <li>Enter your NeSI username, HPC password and 6 digit second factor token (as set on MyNeSI). </li> <li> <p>Choose server options as below. </p> <p>Warning</p> <p>Make sure to choose the correct project code <code>nesi99991</code>, number of CPUs 2, memory 4GB and GPU None prior to pressing the button .</p> </li> <li> <p>Start a terminal session from the JupyterLab launcher. </p> </li> <li>And voil\u00e0! You are ready to go \ud83d\ude04.</li> </ol> <p>Note</p> <p>All commands listed in the workshop will be entered in this terminal.</p>"},{"location":"setup/#workshop-folder","title":"Workshop folder","text":"<p>This workshop involves creating a few files. You should have a folder named with your NeSI login under <code>/nesi/project/nesi99991/introhpc2403/</code>.</p> <p>Just in case, let\u2019s make sure it exists using:</p> <pre><code>mkdir -p /nesi/project/nesi99991/introhpc2403/$USER\n</code></pre> <p>And then move to this folder:</p> <pre><code>cd /nesi/project/nesi99991/introhpc2403/$USER\n</code></pre> <p>In this workshop, we will use the <code>nano</code> editor to edit files. Note that you can also use the JupyterLab file browser and editor instead.</p> <p>In the next section, we will explore multiple ways to access and install deep learning toolboxes.</p>"},{"location":"submit/","title":"Submit Batch Jobs","text":"<p>This section details how to request access to a GPU (graphics processing unit) on NeSI for batch jobs. We will also see how to make sure that the installed deep learning packages properly find and use them.</p>"},{"location":"submit/#available-gpus","title":"Available GPUs","text":"<p>GPU are dedicated piece of hardware filled with specialised compute units to handle massively parallel computations<sup>1</sup>. The massively parallel architecture of deep learning models make them well-suited to run on GPUs, drastically decreasing their training time.</p> <p>NeSI HPC platform gives access to different types of GPUs. Here is a little tour of the available capacity as of March 2024:</p> GPU type Location Access type 9 NVIDIA Tesla P100 PCIe 12GB cards (1 node with 1 GPU, 4 nodes with 2 GPUs) Mahuika Slurm and Jupyter 5 NVIDIA Tesla P100 PCIe 12GB cards (5 nodes with 1 GPU) M\u0101ui Ancil. Slurm 7 A100-1g.5gb instances (1 NVIDIA A100 PCIe 40GB card divided into 7 MIG GPU slices with 5GB memory each) Mahuika Slurm and Jupyter 7 NVIDIA A100 PCIe 40GB cards (4 nodes with 1 GPU, 2 nodes with 2 GPUs) Mahuika Slurm 4 NVIDIA HGX A100 boards (4 GPUs per board with 80GB memory each, 16 A100 GPUs in total) Mahuika Slurm <p>Which one should you use?</p> <ul> <li>for small experimentations, start with a A100-1g.5gb or a P100,</li> <li>if you need to run legacy code (e.g. TensorFlow 1.x) try a P100,</li> <li>otherwise use the PCIe or HGX A100,</li> <li>and if you need large memory and/or multiple GPUs, use the HGX A100s.</li> </ul> Limits on GPU Jobs <ul> <li>Per-project limit of 6 GPUs being used at a time.</li> <li>Per-project limit of 360 GPU-hours being allocated to running jobs.   For example, you can use 6 GPUs at a time if your jobs run for 2 days, but only two GPUs if your jobs run for a week.</li> <li>No more than 64 CPUs per GPU job, to ensure that GPUs are not left idle due to lack of free CPUs.</li> <li>Per-user limit of one A100-1g.5gb GPU job.</li> </ul>"},{"location":"submit/#slurm-job-submission","title":"Slurm job submission","text":"<p>When preparing our Slurm job script, we need to make sure we tell Slurm that we need a GPU, using the <code>--gpus-per-node</code> option. In a job submission script, the syntax is the following:</p> <pre><code>#SBATCH --gpus-per-node=&lt;gpu_type&gt;:&lt;gpu_number&gt;\n</code></pre> <p>Depending on the GPU type, we may also need to specify a partition using <code>--partition</code>.</p> GPU type Slurm option Mahuika P100 <pre><code>#SBATCH --gpus-per-node=P100:1</code></pre> M\u0101ui Ancil. P100 <pre><code>#SBATCH --partition=nesi_gpu#SBATCH --gpus-per-node=1</code></pre> A100-1g.5gb <pre><code>#SBATCH --gpus-per-node=A100-1g.5gb:1</code></pre> PCIe A100 (40GB) <pre><code>#SBATCH --gpus-per-node=A100:1</code></pre> HGX A100 (80GB) <pre><code>#SBATCH --partition=hgx#SBATCH --gpus-per-node=A100:1</code></pre> Any A100 \ud83d\ude80 <pre><code>#SBATCH --partition=hgx,gpu#SBATCH --gpus-per-node=A100:1</code></pre> <p>For today\u2019s exercises, we will use a big one \ud83e\udd2f, an HGX A100 GPU.</p> <p>Let\u2019s start with a very simple batch job, printing simple information about the requested GPU:</p> gpujob.sl<pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=1GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n</code></pre> <p>Create the file <code>gpujob.sl</code> in your workshop folder, for example using the nano editor:</p> <pre><code>cd /nesi/project/nesi99991/introhpc2403/$USER\nnano gpujob.sl\n</code></pre> <p>Then let\u2019s submit the job using <code>sbatch</code>:</p> <pre><code>sbatch gpujob.sl\n</code></pre> output <pre><code>Submitted batch job 44344744\n</code></pre> <p>You can check the state of all your jobs using <code>squeue --me</code>. Once completed, chech the content of the Slurm log file (replace <code>44344744</code> with your job ID):</p> <pre><code>cat slurm-44344744.out\n</code></pre> output <pre><code>Tue Mar 12 09:38:23 2024\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  On   | 00000000:C7:00.0 Off |                    0 |\n| N/A   35C    P0    62W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\n</code></pre> <p>Note that <code>nvidia-smi</code> and <code>CUDA_VISIBLE_DEVICES</code> both report one GPU.</p> <p>Exercise</p> <ol> <li>Try to request 2 HGX A100 and compare the output of the log file.</li> <li>Remove the <code>--partition</code> and <code>--gpus-per-node</code> options and compare the results.</li> </ol> <p>Warning</p> <p>As we have just seen, Slurm set the environment variable <code>CUDA_VISIBLE_DEVICES</code> for you, so you don\u2019t need to do it.</p>"},{"location":"submit/#tensorflow-example","title":"TensorFlow example","text":"<p>Let\u2019s continue with a more realistic example. We will use the following script to train a small CNN (convolutional neural network) to classify images from the CIFAR-10 dataset using TensorFlow.</p> train_model.py<pre><code>import sys\nfrom pathlib import Path\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\nif len(sys.argv) != 2:\n    raise ValueError(\"missing output folder parameter\")\n\noutput_folder = Path(sys.argv[1])\noutput_folder.mkdir(parents=True, exist_ok=True)\n\n# load training data\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\ninput_shape = train_images.shape[1:]\n\n# create  and compile a CNN model\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation=\"relu\"))\nmodel.add(layers.Dense(10))\n\nprint(model.summary())\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\n# train and evaluate the model\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\n    log_dir=output_folder / \"logs\", histogram_freq=1\n)\nhistory = model.fit(\n    train_images,\n    train_labels,\n    epochs=5,\n    validation_data=(test_images, test_labels),\n    callbacks=[tensorboard_callback]\n)\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"test accuracy: {test_acc}\")\n\n# save final model\nmodel.save(output_folder / \"trained_model_cifar10\")\n</code></pre> <p>This file is available in the workshop folder, let\u2019s make a copy of it \ud83d\ude42:</p> <pre><code>cd /nesi/project/nesi99991/introhpc2403/$USER\ncp /nesi/project/nesi99991/introhpc2403/train_model.py ./\n</code></pre> <p>Let\u2019s now explore how to submit a Slurm job to execute it. The job script will vary depending on the method we use to access TensorFlow.</p>"},{"location":"submit/#environment-module","title":"Environment module","text":"<p>First, let\u2019s try with tne TensorFlow environment module.</p> <p>Let\u2019s adapt our <code>gpujob.sl</code> script to load the environment module and run the script, inserting the following instructions at the end:</p> <pre><code># load required environment modules\nmodule purge\nmodule load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>We also need to increase the memory to 8GB too.</p> <pre><code>cp gpujob.sl train_model_env.sl\nnano train_model_env.sl\n</code></pre> train_model_env.sl <pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=8GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# load required environment modules\nmodule purge\nmodule load TensorFlow/2.13.0-gimkl-2022a-Python-3.11.3\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>Let\u2019s submit this script as a job:</p> <pre><code>sbatch train_model_env.sl\n</code></pre> output <pre><code>Submitted batch job 44348778\n</code></pre> <p>Once completed, a new result folder should have appeared, with the logs and model checkpoint for this run (replace <code>44348778</code> with your job ID):</p> <pre><code>find 44348778_train_model_env.sl/\n</code></pre> output <pre><code>44348778_train_model_env.sl/\n44348778_train_model_env.sl/logs\n44348778_train_model_env.sl/logs/train\n44348778_train_model_env.sl/logs/train/events.out.tfevents.1710266108.wmg001.2340955.0.v2\n44348778_train_model_env.sl/logs/validation\n44348778_train_model_env.sl/logs/validation/events.out.tfevents.1710266115.wmg001.2340955.1.v2\n44348778_train_model_env.sl/trained_model_cifar10\n44348778_train_model_env.sl/trained_model_cifar10/saved_model.pb\n44348778_train_model_env.sl/trained_model_cifar10/fingerprint.pb\n44348778_train_model_env.sl/trained_model_cifar10/keras_metadata.pb\n44348778_train_model_env.sl/trained_model_cifar10/assets\n44348778_train_model_env.sl/trained_model_cifar10/variables\n44348778_train_model_env.sl/trained_model_cifar10/variables/variables.index\n44348778_train_model_env.sl/trained_model_cifar10/variables/variables.data-00000-of-00001\n</code></pre> <p>Let\u2019s now examine the log file to see how the training went (replace <code>44348778</code> with your job ID):</p> <pre><code>cat slurm-44348778.out\n</code></pre> output <pre><code>Tue Mar 12 17:55:02 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n| N/A   30C    P0    63W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n2024-03-12 17:55:04.385113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-12 17:55:07.601961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78945 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:46:00.0, compute capability: 8.0\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n\n max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n D)                                                              \n\n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n\n max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n g2D)                                                            \n\n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n\n flatten (Flatten)           (None, 1024)              0         \n\n dense (Dense)               (None, 64)                65600     \n\n dense_1 (Dense)             (None, 10)                650       \n\n=================================================================\nTotal params: 122570 (478.79 KB)\nTrainable params: 122570 (478.79 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/5\n2024-03-12 17:55:10.015697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n2024-03-12 17:55:10.528644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n2024-03-12 17:55:10.530676: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x153ead5d2f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-03-12 17:55:10.530705: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2024-03-12 17:55:10.534564: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-03-12 17:55:10.637117: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1563/1563 [==============================] - 7s 3ms/step - loss: 1.5294 - accuracy: 0.4401 - val_loss: 1.2523 - val_accuracy: 0.5466\nEpoch 2/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 1.1661 - accuracy: 0.5858 - val_loss: 1.2037 - val_accuracy: 0.5744\nEpoch 3/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 1.0246 - accuracy: 0.6401 - val_loss: 1.0090 - val_accuracy: 0.6460\nEpoch 4/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 0.9348 - accuracy: 0.6713 - val_loss: 0.9410 - val_accuracy: 0.6689\nEpoch 5/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 0.8608 - accuracy: 0.6988 - val_loss: 0.9287 - val_accuracy: 0.6745\n313/313 - 0s - loss: 0.9287 - accuracy: 0.6745 - 352ms/epoch - 1ms/step\ntest accuracy: 0.6744999885559082\n</code></pre> <p>It looks like that everything went well \ud83e\udd73!</p> <p>Tip</p> <p>TensorFlow is usually quite verbose, it is always good to check if it has detected the GPU and is using it. Here, the following line of the log file informs us that it is the case:</p> <pre><code>2024-03-12 17:55:10.530705: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n</code></pre>"},{"location":"submit/#conda-environment","title":"Conda environment","text":"<p>Let\u2019s redo the same thing, but this time using the TensorFlow package installed in our conda environment.</p> <p>To do so, we will need to modify load the <code>Miniconda3</code> module and activate the environment before executing the script:</p> <pre><code># load required environment modules\nmodule purge\nmodule load Miniconda3/22.11.1-1\n\n# activate the conda environment\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\nconda deactivate\nconda activate /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>Let\u2019s copy our <code>gpujob.sl</code> script and adapt it:</p> <pre><code>cp gpujob.sl train_model_conda.sl\nnano train_model_conda.sl\n</code></pre> <p>Don\u2019t forget to increase the memory to 8GB.</p> train_model_conda.sl <pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=8GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# load required environment modules\nmodule purge\nmodule load Miniconda3/22.11.1-1\n\n# activate the conda environment\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\nconda deactivate\nconda activate /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>Let\u2019s run this script:</p> <pre><code>sbatch train_model_conda.sl\n</code></pre> output <pre><code>Submitted batch job 44349122\n</code></pre> <p>Check the job state using <code>squeue --me</code>. It seems to be slower than the version using the environment module \ud83e\udd14.</p> <p>Let\u2019s examinate the log file content (replace <code>44349122</code> with your job ID):</p> <pre><code>cat slurm-44349122.out\n</code></pre> output <pre><code>Tue Mar 12 18:59:59 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n| N/A   30C    P0    63W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n2024-03-12 19:00:01.818851: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-03-12 19:00:01.855127: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-03-12 19:00:01.855498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-12 19:00:02.591725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n2024-03-12 19:00:04.115146: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n\n max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n )                                                               \n\n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n\n max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n 2D)                                                             \n\n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n\n flatten (Flatten)           (None, 1024)              0         \n\n dense (Dense)               (None, 64)                65600     \n\n dense_1 (Dense)             (None, 10)                650       \n\n=================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n1563/1563 [==============================] - 37s 23ms/step - loss: 1.5266 - accuracy: 0.4439 - val_loss: 1.3006 - val_accuracy: 0.5370\nEpoch 2/5\n1563/1563 [==============================] - 35s 22ms/step - loss: 1.1699 - accuracy: 0.5855 - val_loss: 1.0767 - val_accuracy: 0.6192\nEpoch 3/5\n1563/1563 [==============================] - 35s 22ms/step - loss: 1.0244 - accuracy: 0.6394 - val_loss: 0.9981 - val_accuracy: 0.6482\nEpoch 4/5\n1563/1563 [==============================] - 35s 22ms/step - loss: 0.9237 - accuracy: 0.6775 - val_loss: 0.9513 - val_accuracy: 0.6672\nEpoch 5/5\n1563/1563 [==============================] - 35s 22ms/step - loss: 0.8549 - accuracy: 0.7008 - val_loss: 0.9288 - val_accuracy: 0.6801\n313/313 - 2s - loss: 0.9288 - accuracy: 0.6801 - 2s/epoch - 6ms/step\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\ntest accuracy: 0.6801000237464905\n</code></pre> <p>Let\u2019s have a closer look at the messages \ud83d\udd0e.</p> <pre><code>2024-03-12 19:00:01.855127: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n</code></pre> <p>Oh, that\u2019s not good! TensorFlow could not find CUDA and therefore failed to use the GPU.</p> <p>To fix it, we need to load the CUDA (and cuDNN) module and tell TensorFlow where to find it. Edit the <code>train_model_conda.sl</code> script to insert these lines before running <code>train_model.py</code>:</p> <pre><code>module load cuDNN/8.6.0.163-CUDA-11.8.0\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_PATH\n</code></pre> train_model_conda.sl (fixed) <pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=8GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# load required environment modules\nmodule purge\nmodule load Miniconda3/22.11.1-1\nmodule load cuDNN/8.6.0.163-CUDA-11.8.0\n\n# ensures XLA can find CUDA libraries\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_PATH\n\n# activate the conda environment\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1\nconda deactivate\nconda activate /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n\n# execute the script\npython train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>We can now resubmit the job:</p> <pre><code>sbatch train_model_conda.sl \n</code></pre> success <pre><code>Submitted batch job 44349128\n</code></pre> <p>And examine the log file to confirm that now the GPU is used by TensorFlow (replace <code>44349128</code> with your job ID):</p> <pre><code>cat slurm-44349128.out\n</code></pre> output <pre><code>Tue Mar 12 19:07:00 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n| N/A   30C    P0    63W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\n2024-03-12 19:07:02.339469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-12 19:07:03.048716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n2024-03-12 19:07:07.037005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78915 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:46:00.0, compute capability: 8.0\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n\n max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n )                                                               \n\n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n\n max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n 2D)                                                             \n\n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n\n flatten (Flatten)           (None, 1024)              0         \n\n dense (Dense)               (None, 64)                65600     \n\n dense_1 (Dense)             (None, 10)                650       \n\n=================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n2024-03-12 19:07:10.016715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n2024-03-12 19:07:10.526440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n2024-03-12 19:07:10.778501: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x555575f88670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-03-12 19:07:10.778540: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2024-03-12 19:07:10.783568: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-03-12 19:07:10.916067: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1563/1563 [==============================] - 8s 3ms/step - loss: 1.5133 - accuracy: 0.4475 - val_loss: 1.2346 - val_accuracy: 0.5525\nEpoch 2/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 1.1527 - accuracy: 0.5927 - val_loss: 1.0675 - val_accuracy: 0.6208\nEpoch 3/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 0.9922 - accuracy: 0.6524 - val_loss: 1.0370 - val_accuracy: 0.6448\nEpoch 4/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 0.8910 - accuracy: 0.6887 - val_loss: 0.9684 - val_accuracy: 0.6682\nEpoch 5/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.8165 - accuracy: 0.7137 - val_loss: 0.9052 - val_accuracy: 0.6897\n313/313 - 0s - loss: 0.9052 - accuracy: 0.6897 - 361ms/epoch - 1ms/step\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\ntest accuracy: 0.6897000074386597\n</code></pre> <p>Success! Now TensorFlow uses the GPU:</p> <pre><code>2024-03-12 19:07:10.778540: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n</code></pre> <p>and each epoch takes 4s instead of 35s:</p> <pre><code>Epoch 4/5\n1563/1563 [==============================] - 4s 2ms/step - loss: 0.8910 - accuracy: 0.6887 - val_loss: 0.9684 - val_accuracy: 0.6682\n</code></pre> <p>Warning</p> <p>Depending on the version of TensorFlow installed, you will need to load different version of CUDA and cuDNN. Check the TensorFlow compatibility matrix to find which version you need.</p> <p>If the corresponding environment modules do not exist on NeSI, please contact your friendly NeSI support team at support@nesi.org.nz to request it to be installed.</p> <p>Warning</p> <p>For other packages, like PyTorch or Jax, you may need to load CUDA and/or cuDNN. It depends if the CUDA (and cuDNN) toolkit gets installed by <code>conda</code> or <code>pip</code> as dependency, or not.</p> <p>You can check this by looking at the list of packages installed in your conda environment using <code>conda export</code>. Here is an example with the conda environment created for this workshop:</p> <pre><code>module load Miniconda3/23.10.0-1\nconda env export -p /nesi/nobackup/nesi99991/introhpc2403/$USER/venv\n</code></pre>"},{"location":"submit/#apptainer-container","title":"Apptainer container","text":"<p>Finally, let\u2019s try with the container approach.</p> <p>To run our python script using the container, we need to:</p> <ul> <li>load the corresponding environment module   <pre><code>module purge\nmodule load Apptainer/1.2.2\n</code></pre></li> <li>use the <code>apptainer exec</code> command to execute the local script using the interpreter and packages inside the container   <pre><code>apptainer exec --nv /nesi/project/nesi99991/introhpc2403/tensorflow-24.02.sif \\\n  python train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre></li> </ul> <p>Info</p> <ul> <li>The <code>--nv</code> option is needed to expose the GPU inside the container.</li> <li>Containers are expected to have their own copy of the CUDA toolkit inside, therefore we don\u2019t need to the load the CUDA (or cuDNN) environment module.</li> </ul> <p>Let\u2019s create another job submission script (and don\u2019t forget to increase memory to 8GB):</p> <pre><code>cp gpujob.sl train_model_apptainer.sl\nnano train_model_apptainer.sl\n</code></pre> train_model_apptainer.sl <pre><code>#!/usr/bin/env bash\n#SBATCH --account=nesi99991\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=8GB\n#SBATCH --partition=hgx\n#SBATCH --gpus-per-node=A100:1\n\n# display information about the available GPUs\nnvidia-smi\n\n# check the value of the CUDA_VISIBLE_DEVICES variable\necho \"CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# load required environment modules\nmodule purge\nmodule load Apptainer/1.2.2\n\n# execute the script\napptainer exec --nv /nesi/project/nesi99991/introhpc2403/tensorflow-24.02.sif \\\n    python train_model.py \"${SLURM_JOB_ID}_${SLURM_JOB_NAME}\"\n</code></pre> <p>Let\u2019s submit the job:</p> <pre><code>sbatch train_model_apptainer.sl\n</code></pre> output <pre><code>Submitted batch job 44349268\n</code></pre> <p>And look at the log file once the job has completed (replace <code>44349268</code> with your job ID):</p> <pre><code>cat slurm-44349268.out\n</code></pre> output <pre><code>Tue Mar 12 19:33:00 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  On   | 00000000:46:00.0 Off |                    0 |\n| N/A   30C    P0    63W / 400W |      0MiB / 81920MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nCUDA_VISIBLE_DEVICES=0\nThe following modules were not unloaded:\n  (Use \"module --force purge\" to unload all):\n\n  1) XALT/minimal   2) slurm   3) NeSI\nWARNING: SINGULARITY_TMPDIR and APPTAINER_TMPDIR have different values, using the latter\n13:4: not a valid test operator: (\n13:4: not a valid test operator: 525.85.12\n2024-03-12 19:33:13.535779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-12 19:33:13.535849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-12 19:33:13.686115: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-03-12 19:33:13.823985: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-03-12 19:33:22.353297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78867 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:46:00.0, compute capability: 8.0\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        896       \n\n max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n D)                                                              \n\n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n\n max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n g2D)                                                            \n\n conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n\n flatten (Flatten)           (None, 1024)              0         \n\n dense (Dense)               (None, 64)                65600     \n\n dense_1 (Dense)             (None, 10)                650       \n\n=================================================================\nTotal params: 122570 (478.79 KB)\nTrainable params: 122570 (478.79 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/5\n2024-03-12 19:33:25.035720: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:467] Loaded cuDNN version 90000\n2024-03-12 19:33:28.759273: I external/local_xla/xla/service/service.cc:168] XLA service 0x55555bf92310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2024-03-12 19:33:28.760801: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2024-03-12 19:33:28.764827: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710272008.857828 2346162 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1563/1563 [==============================] - 12s 3ms/step - loss: 1.5514 - accuracy: 0.4331 - val_loss: 1.2823 - val_accuracy: 0.5290\nEpoch 2/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 1.1781 - accuracy: 0.5816 - val_loss: 1.1526 - val_accuracy: 0.5874\nEpoch 3/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 1.0207 - accuracy: 0.6384 - val_loss: 1.0523 - val_accuracy: 0.6281\nEpoch 4/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.9263 - accuracy: 0.6742 - val_loss: 0.9380 - val_accuracy: 0.6695\nEpoch 5/5\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.8484 - accuracy: 0.7035 - val_loss: 0.9007 - val_accuracy: 0.6905\n313/313 - 0s - loss: 0.9007 - accuracy: 0.6905 - 403ms/epoch - 1ms/step\ntest accuracy: 0.690500020980835\n</code></pre> <p>Good news, it seems to have worked well \ud83d\ude0e, finding the GPU and using it (4s per epoch):</p> <pre><code>2024-03-12 19:33:28.760801: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n...\n1563/1563 [==============================] - 4s 3ms/step - loss: 0.9263 - accuracy: 0.6742 - val_loss: 0.9380 - val_accuracy: 0.6695\n</code></pre> <p>In the next section, we will have a look at how to monitor the jobs and the model training process.</p> <ol> <li> <p>Recent cards from NVIDIA have even more dedicated processing units, Tensor Cores, that massively accelerate lower-precision matrix-matrix multiplications.\u00a0\u21a9</p> </li> </ol>"}]}